{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more manageable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "Successfully complete all these objectives to earn full credit. \n",
    "\n",
    "**Successful completion is defined as passing all the unit tests in each objective.**  \n",
    "\n",
    "Each unit test that you pass is 1 point. \n",
    "\n",
    "There are 5 total possible points in this sprint challenge. \n",
    "\n",
    "\n",
    "There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews\n",
    "\n",
    "____\n",
    "\n",
    "# Before you submit your notebook you must first\n",
    "\n",
    "1) Restart your notebook's Kernel\n",
    "\n",
    "2) Run all cells sequentially, from top to bottom, so that cell numbers are sequential numbers (i.e. 1,2,3,4,5...)\n",
    "- Easiest way to do this is to click on the **Cell** tab at the top of your notebook and select **Run All** from the drop down menu. \n",
    "\n",
    "3) Comment out the cell that generates a pyLDAvis visual in objective 4 (see instructions in that section). \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bec125eb29f89460cf0c19ba9aa9a2f",
     "grade": false,
     "grade_id": "cell-395851cd95d17235",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load reviews from URL\n",
    "data_url = 'https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_4/unit1_nlp/review_sample.json'\n",
    "\n",
    "# Import data into a DataFrame named df\n",
    "# YOUR CODE HERE\n",
    "\n",
    "df = pd.read_json(data_url, lines=True)\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "356579363f311da83f4ef7abaf3c9212",
     "grade": true,
     "grade_id": "cell-cb5006475e42b8f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible Testing\n",
    "assert isinstance(df, pd.DataFrame), 'df is not a DataFrame. Did you import the data into df?'\n",
    "assert df.shape[0] == 10000, 'DataFrame df has the wrong number of rows.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/U4-S1-NLP/lib/python3.7/site-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n",
      "/anaconda2/envs/U4-S1-NLP/lib/python3.7/site-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n",
      "/anaconda2/envs/U4-S1-NLP/lib/python3.7/site-packages/catalogue.py:126: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n",
      "/anaconda2/envs/U4-S1-NLP/lib/python3.7/site-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n"
     ]
    }
   ],
   "source": [
    "# Optional: Consider using spaCy in your function. The spaCy library can be imported by running this cell.\n",
    "# A pre-trained model (en_core_web_sm) has been made available to you in the CodeGrade container.\n",
    "# If you DON'T need use the en_core_web_sm model, you can comment it out below.\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Went here at 5:30 recently at the suggestion of a downtown hipster friend. Place was small and packed, but had great selection of wines and beers that made the people I was with happy. Personally I'm a cocktail guy, so the beer and wine only deal is always a letdown for me. Service was passable, the place was small but well finished and is a small house divided into a couple of rooms. This is a place trhat you should be prepared to stand at the enitire time, girls with us in heels felt free to bith about that. The front room seats 4 on opposite sides on the room, the ante room seated about three and the back room had a table for eight, I think. That's it, unless you are lucky enough to grab one of the six or eight precious stools at the bar. Interesting mix of professionals and record store employees (pierced and tattaed and etc. and some reasonable eye candy from both camps) who were probably 4 hours early to see some shoe gazers play an off-tune set at Modified. The place has no sign or address number, unless you count the one molded into the planter in the front yard that is invisible from the street, so look carefully while circling the block you'll see it the fourth time around or so. Nice little patio out front but you can't eat, drink, or smoke there or at the one out back - WTF? I see this as a low profile pick-up joint at the right time of day where guys in suits will quietly eye over the hill punk rock girls. Which I like.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Went', 'here', 'at', '530', 'recently', 'at', 'the', 'suggestion', 'of', 'a', 'downtown', 'hipster', 'friend', 'Place', 'was', 'small', 'and', 'packed', 'but', 'had', 'great', 'selection', 'of', 'wines', 'and', 'beers', 'that', 'made', 'the', 'people', 'I', 'was', 'with', 'happy', 'Personally', 'Im', 'a', 'cocktail', 'guy', 'so', 'the', 'beer', 'and', 'wine', 'only', 'deal', 'is', 'always', 'a', 'letdown', 'for', 'me', 'Service', 'was', 'passable', 'the', 'place', 'was', 'small', 'but', 'well', 'finished', 'and', 'is', 'a', 'small', 'house', 'divided', 'into', 'a', 'couple', 'of', 'rooms', 'This', 'is', 'a', 'place', 'trhat', 'you', 'should', 'be', 'prepared', 'to', 'stand', 'at', 'the', 'enitire', 'time', 'girls', 'with', 'us', 'in', 'heels', 'felt', 'free', 'to', 'bith', 'about', 'that', 'The', 'front', 'room', 'seats', '4', 'on', 'opposite', 'sides', 'on', 'the', 'room', 'the', 'ante', 'room', 'seated', 'about', 'three', 'and', 'the', 'back', 'room', 'had', 'a', 'table', 'for', 'eight', 'I', 'think', 'Thats', 'it', 'unless', 'you', 'are', 'lucky', 'enough', 'to', 'grab', 'one', 'of', 'the', 'six', 'or', 'eight', 'precious', 'stools', 'at', 'the', 'bar', 'Interesting', 'mix', 'of', 'professionals', 'and', 'record', 'store', 'employees', 'pierced', 'and', 'tattaed', 'and', 'etc', 'and', 'some', 'reasonable', 'eye', 'candy', 'from', 'both', 'camps', 'who', 'were', 'probably', '4', 'hours', 'early', 'to', 'see', 'some', 'shoe', 'gazers', 'play', 'an', 'offtune', 'set', 'at', 'Modified', 'The', 'place', 'has', 'no', 'sign', 'or', 'address', 'number', 'unless', 'you', 'count', 'the', 'one', 'molded', 'into', 'the', 'planter', 'in', 'the', 'front', 'yard', 'that', 'is', 'invisible', 'from', 'the', 'street', 'so', 'look', 'carefully', 'while', 'circling', 'the', 'block', 'youll', 'see', 'it', 'the', 'fourth', 'time', 'around', 'or', 'so', 'Nice', 'little', 'patio', 'out', 'front', 'but', 'you', 'cant', 'eat', 'drink', 'or', 'smoke', 'there', 'or', 'at', 'the', 'one', 'out', 'back', 'WTF', 'I', 'see', 'this', 'as', 'a', 'low', 'profile', 'pickup', 'joint', 'at', 'the', 'right', 'time', 'of', 'day', 'where', 'guys', 'in', 'suits', 'will', 'quietly', 'eye', 'over', 'the', 'hill', 'punk', 'rock', 'girls', 'Which', 'I', 'like']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def only_alphanum(text):\n",
    "    alphanum = re.sub('[^a-zA-Z 0-9]', '', text)\n",
    "    alphanum = re.sub(r\"\\s+\", \" \", alphanum)\n",
    "    return alphanum\n",
    "\n",
    "print(only_alphanum(df.text[300]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4837ed2a1cc13057ba40203859d46ff6",
     "grade": false,
     "grade_id": "cell-3d570d5a1cd6cb64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    doc = only_alphanum(document) #using our function\n",
    "    doc = nlp(document)\n",
    "    doc_tokens = []\n",
    "    for token in doc:\n",
    "        if (not token.is_punct) & (not token.is_stop) & (token.text != \" \"):\n",
    "            doc_tokens.append(token.lemma_.strip().lower())\n",
    "    return doc_tokens\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2181ca9d36070260b1f75dcfd9e58965",
     "grade": true,
     "grade_id": "cell-02da164f6fbe730a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/U4-S1-NLP/lib/python3.7/site-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n",
      "/anaconda2/envs/U4-S1-NLP/lib/python3.7/site-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n"
     ]
    }
   ],
   "source": [
    "'''Testing'''\n",
    "assert isinstance(tokenize(df.sample(n=1)[\"text\"].iloc[0]), list), \"Make sure your tokenizer function accepts a single document and returns a list of tokens!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews (i.e. create a doc-term matrix).\n",
    "2. Write a fake review and query for the 10 most similar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, use `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df['text'].copy()) #working on a copy of the column with the docs and making it a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d70a0a1a96cf8406c60b17e50b255a1a",
     "grade": false,
     "grade_id": "cell-0e96491cb529202c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 906 ms, total: 2.37 s\n",
      "Wall time: 2.73 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>001695</th>\n",
       "      <th>007</th>\n",
       "      <th>00a</th>\n",
       "      <th>00am</th>\n",
       "      <th>00ish</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>011802</th>\n",
       "      <th>...</th>\n",
       "      <th>誰も乗車しなくても</th>\n",
       "      <th>豆腐花</th>\n",
       "      <th>質問にも丁寧に答えてくれましたし</th>\n",
       "      <th>車好きさんには</th>\n",
       "      <th>這是一個不錯的選擇</th>\n",
       "      <th>運転しない</th>\n",
       "      <th>運転中も英語で指導があります</th>\n",
       "      <th>食べ物はうまい</th>\n",
       "      <th>餐後點了甜點</th>\n",
       "      <th>３時間後の便</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27588 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  001695  007  00a  00am  00ish  00pm  01  011802  ...  誰も乗車しなくても  \\\n",
       "0   0    0       0    0    0     0      0     0   0       0  ...          0   \n",
       "1   0    0       0    0    0     0      0     0   0       0  ...          0   \n",
       "2   0    0       0    0    0     0      0     0   0       0  ...          0   \n",
       "3   0    0       0    0    0     0      0     0   0       0  ...          0   \n",
       "4   0    0       0    0    0     0      0     0   0       0  ...          0   \n",
       "\n",
       "   豆腐花  質問にも丁寧に答えてくれましたし  車好きさんには  這是一個不錯的選擇  運転しない  運転中も英語で指導があります  食べ物はうまい  \\\n",
       "0    0                 0        0          0      0               0        0   \n",
       "1    0                 0        0          0      0               0        0   \n",
       "2    0                 0        0          0      0               0        0   \n",
       "3    0                 0        0          0      0               0        0   \n",
       "4    0                 0        0          0      0               0        0   \n",
       "\n",
       "   餐後點了甜點  ３時間後の便  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "2       0       0  \n",
       "3       0       0  \n",
       "4       0       0  \n",
       "\n",
       "[5 rows x 27588 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Create a vector representation of the reviews \n",
    "# Name that doc-term matrix \"dtm\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def generate_dtm_CVectorizer(corpus):\n",
    "    vect = CountVectorizer()\n",
    "    dtm = vect.fit_transform(corpus)\n",
    "    dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
    "    return dtm\n",
    "\n",
    "def generate_dtm_tfidf(corpus):\n",
    "    tfidf = TfidfVectorizer(stop_words='english', max_features=5000, \n",
    "                        ngram_range=(1,2), max_df=.98, min_df=.01, tokenizer=tokenize) #using the doc tokenizer we built\n",
    "\n",
    "    dtm = tfidf.fit_transform(corpus) \n",
    "\n",
    "    dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
    "    \n",
    "    return dtm\n",
    "\n",
    "def get_nearest(dtm, dtm_index, my_nn):\n",
    "    dtm_doc = [dtm.iloc[dtm_index].values]\n",
    "    neigh_dist, neigh_index = my_nn.kneighbors(dtm_doc)\n",
    "    df = pd.DataFrame(list(zip(neigh_index[0], neigh_dist[0])), columns=['index', 'cos_sim'])\n",
    "    return df\n",
    "    \n",
    "def nearest_from_text(generator, corpus , text):\n",
    "    my_corpus = corpus.copy()\n",
    "    my_corpus.append(text)\n",
    "    dtm = generator(my_corpus)\n",
    "    nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree') #we are aiming at 10 nn so we set param at 11\n",
    "    nn.fit(dtm)\n",
    "    df = get_nearest(dtm, -1, nn)\n",
    "    return df\n",
    "\n",
    "dtm = generate_dtm_CVectorizer(corpus)\n",
    "dtm.head()\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.columns[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32b220e23c9aa1f602f08d1c2e879d0a",
     "grade": false,
     "grade_id": "cell-3d5bc610a8ec6b24",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit a NearestNeighbors model named \"nn\"\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree') #we are aiming at 10 nn so we set param at 11\n",
    "nn.fit(dtm)\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d270ed23df3c7d3c6cf08ab174ccaf9e",
     "grade": true,
     "grade_id": "cell-c43704dcff67e99b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''Testing.'''\n",
    "assert nn.__module__ == 'sklearn.neighbors._unsupervised', ' nn is not a NearestNeighbors instance.'\n",
    "assert nn.n_neighbors == 10, 'nn has the wrong value for n_neighbors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3da2ced9f187ed0aa1a890785e2ba00e",
     "grade": false,
     "grade_id": "cell-496203e8746296ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index   cos_sim\n",
      "0  10000  0.000000\n",
      "1   9898  4.472136\n",
      "2   5129  4.472136\n",
      "3   3543  4.472136\n",
      "4   5087  4.582576\n",
      "5   7352  4.690416\n",
      "6   7553  4.690416\n",
      "7   1356  4.690416\n",
      "8   3405  4.795832\n",
      "9   6810  4.795832\n"
     ]
    }
   ],
   "source": [
    "# Create a fake review and find the 10 most similar reviews\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "my_text = \"It was the best coffee though super expensive for a residential area, severs were excellent and decorations are on spot\"\n",
    "\n",
    "print(nearest_from_text(generate_dtm_CVectorizer, corpus, my_text))\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index    cos_sim\n",
      "0   9999   0.000000\n",
      "1   4355  24.758837\n",
      "2    586  25.000000\n",
      "3   6262  25.396850\n",
      "4   8603  25.436195\n",
      "5   3991  25.514702\n",
      "6   7276  25.632011\n",
      "7   4104  25.690465\n",
      "8   5442  25.748786\n",
      "9      2  25.806976\n"
     ]
    }
   ],
   "source": [
    "print(get_nearest(dtm, -1, nn)) #chacking that we are not overwriting the original values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a pipeline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier.\n",
    "    - Use that pipeline to train a model to predict the `stars` feature (i.e. the labels). \n",
    "    - Use that Pipeline to predict a star rating for your fake review from Part 2. \n",
    "\n",
    "\n",
    "\n",
    "2. Create a parameter dict including `one parameter for the vectorizer` and `one parameter for the model`. \n",
    "    - Include 2 possible values for each parameter\n",
    "    - **Use `n_jobs` = 1** \n",
    "    - Due to limited computational resources on CodeGrader `DO NOT INCLUDE ADDITIONAL PARAMETERS OR VALUES PLEASE.`\n",
    "    \n",
    "    \n",
    "3. Train the entire pipeline with a GridSearch\n",
    "    - Name your GridSearch object as `gs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1d18da8521d51d8bfc4b5b9d005fa34",
     "grade": false,
     "grade_id": "cell-e2beb0252d274bba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   24.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words='english',\n",
       "                                                        strip...\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('class',\n",
       "                                        KNeighborsClassifier(algorithm='auto',\n",
       "                                                             leaf_size=30,\n",
       "                                                             metric='minkowski',\n",
       "                                                             metric_params=None,\n",
       "                                                             n_jobs=None,\n",
       "                                                             n_neighbors=5, p=2,\n",
       "                                                             weights='uniform'))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=1,\n",
       "             param_grid={'class__n_neighbors': (3, 5),\n",
       "                         'vect__max_df': (0.75, 0.95)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Name the gridsearch instance \"gs\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "features = corpus #keep working with the copy \"list format\" of df['text']and not the original column as a preference\n",
    "target = df['stars'].copy()\n",
    "\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', tokenizer=None) #no tokenizer func\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"vect\", tfidf),\n",
    "    (\"class\", knc)\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df':(0.75, 0.95),\n",
    "    #'vect__min_df':(0.01, 0.05),\n",
    "    'class__n_neighbors':(3,5),\n",
    "    #'class__leaf_size':(25,40)\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=parameters, n_jobs=1, cv=3, verbose=1)\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.75, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('class',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=5, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = gs.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9e2378efb868f104a4eb39e4f25563c",
     "grade": true,
     "grade_id": "cell-d07134c6fe5d056e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Visible Testing\n",
    "prediction = gs.predict([\"I wish dogs knew how to speak English.\"])[0]\n",
    "print (prediction)\n",
    "prediction = gs.predict([my_text])[0] #predicting my random review\n",
    "print (prediction)\n",
    "\n",
    "assert prediction in df.stars.values, 'You gs object should be able to accept raw text within a list. Did you include a vectorizer in your pipeline?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Set num_topics to `5`\n",
    "    - Name your LDA model `lda`\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "When you instantiate your LDA model, it should look like this: \n",
    "\n",
    "```python\n",
    "lda = LdaModel(corpus=corpus,\n",
    "               id2word=id2word,\n",
    "               random_state=723812,\n",
    "               num_topics = num_topics,\n",
    "               passes=1\n",
    "              )\n",
    "\n",
    "```\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about  pyLDAvis\n",
    "\n",
    "**pyLDAvis** is the Topic modeling package that we used in class to visualize the topics that LDA generates for us.\n",
    "\n",
    "You are welcomed to use pyLDAvis if you'd like for your visualization. However, **you MUST comment out the code that imports the package and the cell that generates the visualization before you submit your notebook to CodeGrade.** \n",
    "\n",
    "Although you should leave the print out of the visualization for graders to see (i.e. comment out the cell after you run it to create the viz). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "# Due to limited computationalresources on CodeGrader, use the non-multicore version of LDA \n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimate a LDA topic model of the review tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9514841e71735eaa255bccc53b257896",
     "grade": false,
     "grade_id": "cell-66331a185ff52f15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember to read the LDA docs for more information on the various class attirbutes and methods available to you\n",
    "# in the LDA model: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "# don't change this value \n",
    "num_topics = 5\n",
    "\n",
    "# use tokenize function you created earlier to create tokens \n",
    "\n",
    "tokens = df['text'].apply(tokenize) \n",
    "\n",
    "def filter_tokens(doc):\n",
    "    doc_tokens = []\n",
    "    for token in doc:\n",
    "        if len(token) > 2:\n",
    "            doc_tokens.append(token)\n",
    "    return doc_tokens\n",
    "\n",
    "tokens = tokens.apply(filter_tokens)\n",
    "\n",
    "# create a id2word object (hint: use corpora.Dictionary)\n",
    "id2word = corpora.Dictionary(tokens)\n",
    "# create a corpus object (hint: id2word.doc2bow)\n",
    "corpus = [id2word.doc2bow(text) for text in tokens]\n",
    "# instantiate an lda model\n",
    "lda = gensim.models.ldamulticore.LdaMulticore(corpus=corpus)\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6479db0fa59c99d3ae3201c1f10ebca1",
     "grade": true,
     "grade_id": "cell-5a3c181311134fa9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible Testing\n",
    "assert lda.get_topics().shape[0] == 5, 'Did your model complete its training? Did you set num_topics to 5?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "189591ed7b9e6e6146d59761fb418268",
     "grade": false,
     "grade_id": "cell-9b043e992fbd218c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use pyLDAvis (or a ploting tool of your choice) to visualize your results \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f44a26c754500ff0bf585296075bf754",
     "grade": false,
     "grade_id": "cell-bf9e63d9645bba84",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### 3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
